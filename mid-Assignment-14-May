ReplicaSet

1. ReplicaSet has matchLabels, whereas ReplicationController its not required
2. 
[root@ip-172-31-24-167 05-services]# kubectl get pods
No resources found in varun namespace.
[root@ip-172-31-24-167 05-services]# kubectl apply -f kubia-replicaset.yaml
replicaset.apps/kubia created
[root@ip-172-31-24-167 05-services]# kubectl get pods
NAME          READY   STATUS              RESTARTS   AGE
kubia-8npfp   0/1     ContainerCreating   0          2s
kubia-t82qb   0/1     ContainerCreating   0          2s
kubia-txglx   0/1     ContainerCreating   0          2s
[root@ip-172-31-24-167 05-services]# kubectl get pods --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-8npfp   1/1     Running   0          68s   app=kubia
kubia-t82qb   1/1     Running   0          68s   app=kubia
kubia-txglx   1/1     Running   0          68s   app=kubia
[root@ip-172-31-24-167 04-controllers]# kubectl get rs
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       4m
[root@ip-172-31-24-167 04-controllers]# kubectl scale rs kubia --replicas=2
replicaset.apps/kubia scaled
[root@ip-172-31-24-167 04-controllers]# kubectl get pods --show-labels
NAME          READY   STATUS        RESTARTS   AGE     LABELS
kubia-8npfp   1/1     Running       0          4m50s   app=kubia
kubia-t82qb   1/1     Running       0          4m50s   app=kubia
kubia-txglx   0/1     Terminating   0          4m50s   app=kubia
[root@ip-172-31-24-167 05-services]# kubectl apply -f kubia-svc.yaml
service/kubia2 created
[root@ip-172-31-24-167 05-services]# kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-8npfp   1/1     Running   0          6m22s
kubia-t82qb   1/1     Running   0          6m22s
[root@ip-172-31-24-167 05-services]# kubectl get svc
NAME     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubia2   ClusterIP   10.99.10.99   <none>        80/TCP    11s
[root@ip-172-31-24-167 05-services]# curl 10.99.10.99:80
You've hit kubia-t82qb
[root@ip-172-31-24-167 05-services]# kubectl label rs kubia app=varun --overwrite
replicaset.apps/kubia labeled
[root@ip-172-31-24-167 05-services]# kubectl get pods --show-labels
NAME          READY   STATUS              RESTARTS   AGE     LABELS
kubia-8npfp   1/1     Running             0          16m     app=varun
kubia-kbkvg   1/1     Running             0          8m30s   app=kubia
kubia-t82qb   1/1     Running             0          16m     app=kubia
kubia-whdp5   0/1     ContainerCreating   0          3s      app=kubia


Cronjob
[root@ip-172-31-24-167 04-controllers]# kubectl apply -f cronjob.yaml
cronjob.batch/batch-job-every-fifteen-minutes created
[root@ip-172-31-24-167 04-controllers]# kubectl get cj
NAME                              SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE
batch-job-every-fifteen-minutes   0,15,30,45 * * * *   False     0        <none>          22s
[root@ip-172-31-24-167 04-controllers]# kubectl get po
NAME                                               READY   STATUS      RESTARTS   AGE
batch-job-every-fifteen-minutes-1652719500-grmsf   0/1     Completed   0          12m
NAME                              SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE
batch-job-every-fifteen-minutes   0,15,30,45 * * * *   False     0        12m             13m
Cronjobs run in a scheduled manner, whereas job created on demand and terminated


Service
verify kubectl get svc
kubectl describe svc
Check port number mapped in kubectl get pods -owide, verify they are matching with the ones attached to the svc yaml

VOTING APP ASSIGNMENT
Everytime the pod is deleted, new pod comes up, keeping the application running at all time and providing high availibility. On db deletion, result app was not being updated untill the new pod came up (it wont work as the db holding the data was down)
Terminologies learnt:
 Kubectl
 container
 swarm
 cluster
 charts
 annotations
 controller
 image
 kube-proxy
 pod
 namespace
